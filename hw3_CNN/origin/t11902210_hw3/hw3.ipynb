{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HW3 Image Classification\n#### Solve image classification with convolutional neural networks(CNN).\n#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com","metadata":{}},{"cell_type":"markdown","source":"# Kaggle code","metadata":{}},{"cell_type":"markdown","source":"# Reference link （all） \nFor specific location references, see the markdown and comments in the code below.\n\nhttps://www.cnblogs.com/lccxqk/p/14601537.html\n\nhttps://blog.csdn.net/iwill323/article/details/127894848\n\nhttp://www.360doc.com/content/21/0126/16/73546223_959051784.shtml\n\nhttps://blog.csdn.net/Raphael9900/article/details/128207600","metadata":{}},{"cell_type":"code","source":"# check GPU type.\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:47.038751Z","iopub.execute_input":"2023-03-22T11:39:47.039118Z","iopub.status.idle":"2023-03-22T11:39:48.212798Z","shell.execute_reply.started":"2023-03-22T11:39:47.039085Z","shell.execute_reply":"2023-03-22T11:39:48.211568Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Wed Mar 22 11:39:48 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   41C    P0    32W / 250W |   1579MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Import Packages","metadata":{}},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.215521Z","iopub.execute_input":"2023-03-22T11:39:48.216177Z","iopub.status.idle":"2023-03-22T11:39:48.221574Z","shell.execute_reply.started":"2023-03-22T11:39:48.216132Z","shell.execute_reply":"2023-03-22T11:39:48.220343Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.223575Z","iopub.execute_input":"2023-03-22T11:39:48.223950Z","iopub.status.idle":"2023-03-22T11:39:48.235771Z","shell.execute_reply.started":"2023-03-22T11:39:48.223913Z","shell.execute_reply":"2023-03-22T11:39:48.234736Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.238722Z","iopub.execute_input":"2023-03-22T11:39:48.239217Z","iopub.status.idle":"2023-03-22T11:39:48.248134Z","shell.execute_reply.started":"2023-03-22T11:39:48.239178Z","shell.execute_reply":"2023-03-22T11:39:48.247033Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{}},{"cell_type":"markdown","source":"# reference link\nhttp://www.360doc.com/content/21/0126/16/73546223_959051784.shtml\nhttps://blog.csdn.net/Raphael9900/article/details/128207600","metadata":{}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    #transforms.Resize((128, 128)),\n    # You may add some transforms here.\n    #reference link\n    #http://www.360doc.com/content/21/0126/16/73546223_959051784.shtml\n    #https://blog.csdn.net/Raphael9900/article/details/128207600\n    \n    transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.5),\n    transforms.RandomRotation(180),\n    transforms.RandomAffine(30),\n    transforms.RandomGrayscale(p=0.2),\n\n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.251282Z","iopub.execute_input":"2023-03-22T11:39:48.251584Z","iopub.status.idle":"2023-03-22T11:39:48.261121Z","shell.execute_reply.started":"2023-03-22T11:39:48.251535Z","shell.execute_reply":"2023-03-22T11:39:48.260057Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return im,label","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.263679Z","iopub.execute_input":"2023-03-22T11:39:48.264112Z","iopub.status.idle":"2023-03-22T11:39:48.276275Z","shell.execute_reply.started":"2023-03-22T11:39:48.264077Z","shell.execute_reply":"2023-03-22T11:39:48.275224Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"markdown","source":"# Reference link\n殘差神經網路\n\nhttps://www.cnblogs.com/lccxqk/p/14601537.html\n\nhttps://blog.csdn.net/iwill323/article/details/127894848","metadata":{}},{"cell_type":"code","source":"# Reference link\n#殘差神經網路\n#https://www.cnblogs.com/lccxqk/p/14601537.html\n#https://blog.csdn.net/iwill323/article/details/127894848\nclass Residual_Block(nn.Module):\n    def __init__(self, ic, oc, stride=1):\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        super().__init__()\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(ic, oc, kernel_size=3, stride=stride, padding=1),\n            nn.BatchNorm2d(oc),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(oc, oc, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(oc),\n        )\n        \n        self.relu = nn.ReLU(inplace=True)\n    \n        self.downsample = None\n        if stride != 1 or (ic != oc):\n            self.downsample = nn.Sequential(\n                nn.Conv2d(ic, oc, kernel_size=1, stride=stride),\n                nn.BatchNorm2d(oc),\n            )\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.conv2(out)\n        \n        if self.downsample:\n            residual = self.downsample(x)\n            \n        out += residual\n        return self.relu(out)\n\n    \nclass Classifier(nn.Module):\n    def __init__(self, block, num_layers, num_classes=11):\n        super().__init__()\n        self.preconv = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n        )\n        \n        self.layer0 = self.make_residual(block, 32, 64,  num_layers[0], stride=2)\n        self.layer1 = self.make_residual(block, 64, 128, num_layers[1], stride=2)\n        self.layer2 = self.make_residual(block, 128, 256, num_layers[2], stride=2)\n        self.layer3 = self.make_residual(block, 256, 512, num_layers[3], stride=2)\n        \n        #self.avgpool = nn.AvgPool2d(2)\n        \n        self.fc = nn.Sequential(            \n            nn.Dropout(0.4),\n            nn.Linear(512*4*4, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(512, 11),\n        )\n        \n        \n    def make_residual(self, block, ic, oc, num_layer, stride=1):\n        layers = []\n        layers.append(block(ic, oc, stride))\n        for i in range(1, num_layer):\n            layers.append(block(oc, oc))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        # [3, 128, 128]\n        out = self.preconv(x)  # [32, 64, 64]\n        out = self.layer0(out) # [64, 32, 32]\n        out = self.layer1(out) # [128, 16, 16]\n        out = self.layer2(out) # [256, 8, 8]\n        out = self.layer3(out) # [512, 4, 4]\n        #out = self.avgpool(out) # [512, 2, 2]\n        out = self.fc(out.view(out.size(0), -1)) \n        return out\n    \nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass FocalLoss(nn.Module):\n    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n        super().__init__()\n        if alpha is None:\n            self.alpha = Variable(torch.ones(class_num, 1))\n        else:\n            if isinstance(alpha, Variable):\n                self.alpha = alpha\n            else:\n                self.alpha = Variable(alpha)\n        self.gamma = gamma\n        self.class_num = class_num\n        self.size_average = size_average\n        \n    def forward(self, inputs, targets):\n        N = inputs.size(0)\n        C = inputs.size(1)\n        P = F.softmax(inputs, dim=1)\n        \n        class_mask = inputs.data.new(N, C).fill_(0)\n        class_mask = Variable(class_mask)\n        ids = targets.view(-1, 1)\n        class_mask.scatter_(1, ids.data, 1.)\n        \n        if inputs.is_cuda and not self.alpha.is_cuda:\n            self.alpha = self.alpha.cuda()\n        alpha = self.alpha[ids.data.view(-1)]\n        probs = (P*class_mask).sum(1).view(-1, 1)\n        \n        log_p = probs.log()\n        \n        batch_loss = -alpha*(torch.pow((1-probs), self.gamma))*log_p\n        \n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss.sum()\n            \n        return loss\n    \nclass MyCrossEntropy(nn.Module):\n    def __init__(self, class_num):\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.277934Z","iopub.execute_input":"2023-03-22T11:39:48.278582Z","iopub.status.idle":"2023-03-22T11:39:48.303052Z","shell.execute_reply.started":"2023-03-22T11:39:48.278528Z","shell.execute_reply":"2023-03-22T11:39:48.301996Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"markdown","source":"# reference link\n交叉驗證設定參數\n\nhttps://blog.csdn.net/iwill323/article/details/127894848","metadata":{}},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\n#model = Classifier().to(device)\n\n# reference link\n#交叉驗證設定參數\n\n#https://blog.csdn.net/iwill323/article/details/127894848\n\n# The number of batch size.\nbatch_size = 64\nnum_layers = [2, 4, 3, 1]\n# The number of training epochs.\nn_epochs = 300\nalpha = torch.Tensor([1, 2.3, 0.66, 1, 1.1, 0.75, 2.3, 3.5, 1.1, 0.66, 1.4])\n# If no improvement in 'patience' epochs, early stop.\npatience = 32\nk_fold = 4\n\n# For the classification task, we use cross-entropy as the measurement of performance.\n#criterion = nn.CrossEntropyLoss()\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n#optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.306244Z","iopub.execute_input":"2023-03-22T11:39:48.306626Z","iopub.status.idle":"2023-03-22T11:39:48.319582Z","shell.execute_reply.started":"2023-03-22T11:39:48.306592Z","shell.execute_reply":"2023-03-22T11:39:48.318626Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{}},{"cell_type":"markdown","source":"# reference link\n交叉驗證\n\nhttps://blog.csdn.net/iwill323/article/details/127894848","metadata":{}},{"cell_type":"code","source":"# Construct train and valid datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntrain_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/train\", tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n\ntrain_dir = \"/kaggle/input/ml2023spring-hw3/train\"\nval_dir = \"/kaggle/input/ml2023spring-hw3/valid\"\ntrain_files = [os.path.join(train_dir, x) for x in os.listdir(train_dir) if x.endswith('.jpg')]\nval_files = [os.path.join(val_dir, x) for x in os.listdir(val_dir) if x.endswith('.jpg')]\ntotal_files = train_files + val_files\nrandom.shuffle(total_files)\n\nnum = len(total_files) // k_fold","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.321182Z","iopub.execute_input":"2023-03-22T11:39:48.321758Z","iopub.status.idle":"2023-03-22T11:39:48.441881Z","shell.execute_reply.started":"2023-03-22T11:39:48.321718Z","shell.execute_reply":"2023-03-22T11:39:48.440585Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"### Start Training","metadata":{}},{"cell_type":"markdown","source":"# reference link\nhttps://blog.csdn.net/Raphael9900/article/details/128207600","metadata":{}},{"cell_type":"code","source":"# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\n\ntest_fold = k_fold\n\nfor i in range(test_fold):\n    fold = i+1\n    print(f'\\n\\nStarting Fold: {fold} ********************************************')\n    model = Classifier(Residual_Block, num_layers).to(device)\n    criterion = FocalLoss(11, alpha=alpha)\n    # reference link\n    #https://blog.csdn.net/Raphael9900/article/details/128207600\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0004, weight_decay=2e-5) \n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=16, T_mult=1)\n    stale = 0\n    best_acc = 0\n    \n    val_data = total_files[i*num: (i+1)*num]\n    train_data = total_files[:i*num] + total_files[(i+1)*num:]\n    \n   \n    \n    for epoch in range(n_epochs):\n    \n        # ---------- Training ----------\n        # Make sure the model is in train mode before training.\n        model.train()\n    \n        # These are used to record information in training.\n        train_loss = []\n        train_accs = []\n        lr = optimizer.param_groups[0][\"lr\"]\n        \n        pbar = tqdm(train_loader)\n        pbar.set_description(f'T: {epoch+1:03d}/{n_epochs:03d}')\n        for batch in pbar:\n    \n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n            #imgs = imgs.half()\n            #print(imgs.shape,labels.shape)\n    \n            # Forward the data. (Make sure data and model are on the same device.)\n            logits = model(imgs.to(device))\n    \n            # Calculate the cross-entropy loss.\n            # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n            loss = criterion(logits, labels.to(device))\n    \n            # Gradients stored in the parameters in the previous step should be cleared out first.\n            optimizer.zero_grad()\n    \n            # Compute the gradients for parameters.\n            loss.backward()\n    \n            # Clip the gradient norms for stable training.\n            grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n    \n            # Update the parameters with computed gradients.\n            optimizer.step()\n    \n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n    \n            # Record the loss and accuracy.\n            train_loss.append(loss.item())\n            train_accs.append(acc)\n            pbar.set_postfix({'lr':lr, 'b_loss':loss.item(), 'b_acc':acc.item(),\n                    'loss':sum(train_loss)/len(train_loss), 'acc': sum(train_accs).item()/len(train_accs)})\n        \n        scheduler.step()\n        \n        \n        # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n        model.eval()\n    \n        # These are used to record information in validation.\n        valid_loss = []\n        valid_accs = []\n    \n        # Iterate the validation set by batches.\n        pbar = tqdm(valid_loader)\n        pbar.set_description(f'V: {epoch+1:03d}/{n_epochs:03d}')\n        for batch in pbar:\n\n            # A batch consists of image data and corresponding labels.\n            imgs, labels = batch\n            #imgs = imgs.half()\n    \n            # We don't need gradient in validation.\n            # Using torch.no_grad() accelerates the forward process.\n            with torch.no_grad():\n                logits = model(imgs.to(device))\n    \n            # We can still compute the loss (but not the gradient).\n            loss = criterion(logits, labels.to(device))\n    \n            # Compute the accuracy for current batch.\n            acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n    \n            # Record the loss and accuracy.\n            valid_loss.append(loss.item())\n            valid_accs.append(acc)\n            pbar.set_postfix({'v_loss':sum(valid_loss)/len(valid_loss), \n                              'v_acc': sum(valid_accs).item()/len(valid_accs)})\n        \n            #break\n    \n        # The average loss and accuracy for entire validation set is the average of the recorded values.\n        valid_loss = sum(valid_loss) / len(valid_loss)\n        valid_acc = sum(valid_accs) / len(valid_accs)\n    \n    \n        if valid_acc > best_acc:\n            print(f\"Best model found at fold {fold} epoch {epoch+1}, acc={valid_acc:.5f}, saving model\")\n            torch.save(model.state_dict(), f\"Fold_{fold}_best.ckpt\")\n            # only save best to prevent output memory exceed error\n            best_acc = valid_acc\n            stale = 0\n        else:\n            stale += 1\n            if stale > patience:\n                print(f\"No improvment {patience} consecutive epochs, early stopping\")\n                break","metadata":{"execution":{"iopub.status.busy":"2023-03-22T11:39:48.446011Z","iopub.execute_input":"2023-03-22T11:39:48.447127Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"\n\nStarting Fold: 1 ********************************************\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"025ea62a2002464580dd0ba21490f3b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b47192eadf2544579dafb7a71cbb8e15"}},"metadata":{}},{"name":"stdout","text":"Best model found at fold 1 epoch 1, acc=0.20500, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd764d7bacea4094a0212d47a121bd76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acf75c820e404133a3f3445a71c1175b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"427fe57062054ee7a50654b5f4c1d208"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d0cef7d541b40ffbb888769cd821a2e"}},"metadata":{}},{"name":"stdout","text":"Best model found at fold 1 epoch 3, acc=0.32655, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27871b10157648268163fd57e72bdbc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a5c85f69f8844ed8418f9e28f856eff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddbe731fb0d742498ca8c834578300b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"685bb0c5d7e64948b78cdb49a0d76f34"}},"metadata":{}},{"name":"stdout","text":"Best model found at fold 1 epoch 5, acc=0.36469, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be8045c53e44953ab4283f633f7131f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2191bfe1bab24ffbb021d28eb05c52a7"}},"metadata":{}},{"name":"stdout","text":"Best model found at fold 1 epoch 6, acc=0.41076, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19d1a1d633a74010981a2c789a5cec75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bc7ad5173db4dd69932ed8cf56d9459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"570bd6a5fcb840c6a5a0b6e47c70ce44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14c5734da6ad4cb58300303fd8c3fd71"}},"metadata":{}},{"name":"stdout","text":"Best model found at fold 1 epoch 8, acc=0.43861, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff94acbb01344538f53ea1fc5c16c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a23bf9c1f0314c72ab46e6f8c0135a98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e595867fc94e4b3fb77eea1bea94544e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f6c73521124d62b71cdc24cd835b87"}},"metadata":{}},{"name":"stdout","text":"Best model found at fold 1 epoch 10, acc=0.50673, saving model\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce5c3eaa43a437895fcf1b20194b47e"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Dataloader for test","metadata":{}},{"cell_type":"code","source":"# Construct test datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntest_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/test\", tfm=test_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\ntest_dir = \"/kaggle/input/ml2023spring-hw3/test\"\n#test_tfms = [test_tfm1, test_tfm2, test_tfm3, test_tfm4, test_tfm5]\ntest_loaders = []\nfor i in range(5):\n    test_set_i = FoodDataset(test_dir, tfm=train_tfm)\n    test_loader_i = DataLoader(test_set_i, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n    test_loaders.append(test_loader_i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and generate prediction CSV","metadata":{}},{"cell_type":"markdown","source":"# reference link\n交叉驗證结果\n\nhttps://blog.csdn.net/iwill323/article/details/127894848","metadata":{}},{"cell_type":"code","source":"#model_best = Classifier().to(device)\n#model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n#model_best.eval()\n#prediction = []\n#with torch.no_grad():\n #   for data,_ in tqdm(test_loader):\n #       test_pred = model_best(data.to(device))\n #       test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n  #      prediction += test_label.squeeze().tolist()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\nmodels = []\nfor i in range(0, 4):\n    fold = i + 1\n    model_best = Classifier(Residual_Block, num_layers).to(device)\n    model_best.load_state_dict(torch.load(f\"Fold_{fold}_best.ckpt\"))\n    model_best.eval()\n    models.append(model_best)\n\npreds = [[], [], [], [], [], []] \nwith torch.no_grad():\n    for data, _ in test_loader:\n        batch_preds = [] \n        for model_best in models:\n            batch_preds.append(model_best(data.to(device)).cpu().data.numpy())\n        batch_preds = sum(batch_preds)\n        preds[0].extend(batch_preds.squeeze().tolist())\n        \n    for i, loader in enumerate(test_loaders):\n        for data, _ in loader:\n            batch_preds = []\n            for model_best in models:\n                batch_preds.append(model_best(data.to(device)).cpu().data.numpy())\n            batch_preds = sum(batch_preds)\n            preds[i+1].extend(batch_preds.squeeze().tolist())\n\npreds = np.array(preds)\nprint(preds.shape)\npreds = 0.6* preds[0] + 0.1 * preds[1] + 0.1 * preds[2] + 0.1 * preds[3] + 0.1 * preds[4] + 0.1 * preds[5]\nprint(preds.shape)\nprediction = np.argmax(preds, axis=1)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(len(test_set))]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q1. Augmentation Implementation\n### Implement augmentation by finishing train_tfm in the code with image size of your choice. \n### Directly copy the following block and paste it on GradeScope after you finish the code\n#### Your train_tfm must be capable of producing 5+ different results when given an identical image multiple times.\n#### Your  train_tfm in the report can be different from train_tfm in your training code.","metadata":{}},{"cell_type":"markdown","source":"# reference link\nhttp://www.360doc.com/content/21/0126/16/73546223_959051784.shtml\nhttps://blog.csdn.net/Raphael9900/article/details/128207600","metadata":{}},{"cell_type":"code","source":"train_tfm = transforms.Compose([\n    #reference link\n    #http://www.360doc.com/content/21/0126/16/73546223_959051784.shtml\n    #https://blog.csdn.net/Raphael9900/article/details/128207600\n    # Resize the image into a fixed shape (height = width = 128)\n    #transforms.Resize((128, 128)),\n    # You can add some transforms here.\n    transforms.RandomResizedCrop((128, 128), scale=(0.7, 1.0)),\n    #transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    transforms.RandomHorizontalFlip(0.5),\n    transforms.RandomVerticalFlip(0.5),\n    transforms.RandomRotation(180),\n    transforms.RandomAffine(30),\n    #transforms.RandomInvert(p=0.2),\n    transforms.RandomPosterize(bits=2),\n    transforms.RandomSolarize(threshold=192.0, p=0.2),\n    #transforms.RandomEqualize(p=0.2),\n    transforms.RandomGrayscale(p=0.2),\n\n    transforms.ToTensor(),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q2. Visual Representations Implementation\n### Visualize the learned visual representations of the CNN model on the validation set by implementing t-SNE (t-distributed Stochastic Neighbor Embedding) on the output of both top & mid layers (You need to submit 2 images). \n### ChatGPT has generated the following code, which requires a minor modification to produce the expected results.","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport torch.nn as nn\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Load the trained model\nmodel = Classifier(Residual_Block, num_layers).to(device)\nstate_dict = torch.load(f\"Fold_2_best.ckpt\")\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\nprint(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the vaildation set defined by TA\nvalid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n\n# Extract the representations for the specific layer of model\nindex = 4 # You should find out the index of layer which is defined as \"top\" or 'mid' layer of your model.\nfeatures = []\nlabels = []\nfor batch in tqdm(valid_loader):\n    imgs, lbls = batch\n    with torch.no_grad():\n        logits = model.preconv[:index](imgs.to(device))\n        logits = logits.view(logits.size()[0], -1)\n    labels.extend(lbls.cpu().numpy())\n    logits = np.squeeze(logits.cpu().numpy())\n    features.extend(logits)\n    \nfeatures = np.array(features)\ncolors_per_class = cm.rainbow(np.linspace(0, 1, 11))\n\n# Apply t-SNE to the features\nfeatures_tsne = TSNE(n_components=2, init='pca', random_state=42).fit_transform(features)\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(10, 8))\nfor label in np.unique(labels):\n    plt.scatter(features_tsne[labels == label, 0], features_tsne[labels == label, 1], label=label, s=5)\nplt.legend()\nplt.savefig(\"middle.png\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}