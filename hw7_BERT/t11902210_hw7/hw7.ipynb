{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSGDbExff_I"
      },
      "source": [
        "# **Homework 7 - Bert (Question Answering)**\n",
        "\n",
        "If you have any questions, feel free to email us at ntu-ml-2023spring-ta@googlegroups.com\n",
        "\n",
        "\n",
        "\n",
        "Slide:    [Link](https://docs.google.com/presentation/d/15lGUmT8NpLGtoxRllRWCJyQEjhR1Idcei63YHsDckPE/edit#slide=id.g21fff4e9af6_0_13)　Kaggle: [Link](https://www.kaggle.com/competitions/ml2023spring-hw7/host/sandbox-submissions)　Data: [Link](https://drive.google.com/file/d/1YU9KZFhQqW92Lw9nNtuUPg0-8uyxluZ7/view?usp=sharing)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# colab code"
      ],
      "metadata": {
        "id": "lcw8kg_EFjkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# # Reference link （all）\n",
        "For specific location references, see the markdown and comments in the code below.\n",
        "\n",
        "https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large\n",
        "\n",
        "\n",
        "https://blog.csdn.net/qq_42994201/article/details/121442992\n",
        "\n",
        "\n",
        "https://blog.csdn.net/weixin_42369818/article/details/124835855?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168221931016800226541667%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168221931016800226541667&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-124835855-null-null.142^v86^insert_down1,239^v2^insert_chatgpt&utm_term=%E6%9D%8E%E5%AE%8F%E6%AF%85hw7&spm=1018.2226.3001.4187\n",
        "\n",
        "\n",
        "https://blog.csdn.net/orangerfun/article/details/120400247\n",
        "\n",
        "https://gitee.com/yuys0602/NER-BERT-pytorch/"
      ],
      "metadata": {
        "id": "g6HK-VJHFm0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "NYAHsHNbzdKm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1fSAJE2oaC"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPrc4Eie9Yo5",
        "outputId": "582a7115-8bb4-4708-c562-a5dc55293836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1k2BfGrvhk8QRnr9Xvb04oPIKDr1uWFpa\n",
            "To: /content/hw7_data.zip\n",
            "100% 12.1M/12.1M [00:00<00:00, 191MB/s]\n",
            "Archive:  hw7_data.zip\n",
            "  inflating: hw7_train.json          \n",
            "  inflating: hw7_test.json           \n",
            "  inflating: hw7_dev.json            \n",
            "  inflating: hw7_in-context-learning-examples.json  \n",
            "Sun Apr 23 05:34:40 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# download link 1\n",
        "# !gdown --id '1TjoBdNlGBhP_J9C66MOY7ILIrydm7ZCS' --output hw7_data.zip\n",
        "\n",
        "# download link 2 (if above link failed)\n",
        "# !gdown --id '1YU9KZFhQqW92Lw9nNtuUPg0-8uyxluZ7' --output hw7_data.zip\n",
        "\n",
        "# download link 3 (if above link failed)\n",
        "!gdown --id '1k2BfGrvhk8QRnr9Xvb04oPIKDr1uWFpa' --output hw7_data.zip\n",
        "\n",
        "!unzip -o hw7_data.zip\n",
        "\n",
        "# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TevOvhC03m0h"
      },
      "source": [
        "## Install packages\n",
        "\n",
        "Documentation for the toolkit: \n",
        "*   https://huggingface.co/transformers/\n",
        "*   https://huggingface.co/docs/accelerate/index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbxWFX_jpDom",
        "outputId": "720c3921-1cd7-4927-d6df-9daba0eaf3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.26.1 in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (3.11.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (0.13.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.26.1) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate==0.16.0 in /usr/local/lib/python3.9/dist-packages (0.16.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.16.0) (23.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from accelerate==0.16.0) (6.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.16.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from accelerate==0.16.0) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from accelerate==0.16.0) (1.22.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.16.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.16.0) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.16.0) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.16.0) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.16.0) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4.0->accelerate==0.16.0) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.16.0) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate==0.16.0) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4.0->accelerate==0.16.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4.0->accelerate==0.16.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# You are allowed to change version of transformers or use other toolkits\n",
        "!pip install transformers==4.26.1\n",
        "!pip install accelerate==0.16.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle (Fine-tuning)"
      ],
      "metadata": {
        "id": "XapBp31gytyD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGOr_eS3wJJf"
      },
      "source": [
        "## Task description\n",
        "- Chinese Extractive Question Answering\n",
        "  - Input: Paragraph + Question\n",
        "  - Output: Answer\n",
        "\n",
        "- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n",
        "\n",
        "- Todo\n",
        "    - Fine tune a pretrained chinese BERT model\n",
        "    - Change hyperparameters (e.g. doc_stride)\n",
        "    - Apply linear learning rate decay\n",
        "    - Try other pretrained models\n",
        "    - Improve preprocessing\n",
        "    - Improve postprocessing\n",
        "- Training tips\n",
        "    - Automatic mixed precision\n",
        "    - Gradient accumulation\n",
        "    - Ensemble\n",
        "\n",
        "- Estimated training time (tesla t4 with automatic mixed precision enabled)\n",
        "    - Simple: 8mins\n",
        "    - Medium: 8mins\n",
        "    - Strong: 25mins\n",
        "    - Boss: 2hrs\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dKM4yCh4LI_"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WOTHHtWJoahe"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "from transformers import AdamW\n",
        "\n",
        "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "def same_seeds(seed):\n",
        "\ttorch.manual_seed(seed)\n",
        "\tif torch.cuda.is_available():\n",
        "\t\t\ttorch.cuda.manual_seed(seed)\n",
        "\t\t\ttorch.cuda.manual_seed_all(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\trandom.seed(seed)\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "same_seeds(1370)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YgXHuVLp_6j"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model reference\n",
        "https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large"
      ],
      "metadata": {
        "id": "xHoolo9bkDng"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xyBCYGjAp3ym"
      },
      "outputs": [],
      "source": [
        "#from transformers import (\n",
        "#  AutoTokenizer,\n",
        "#  AutoModelForQuestionAnswering,\n",
        "#)\n",
        "\n",
        "#model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-chinese\").to(device)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
        "\n",
        "model_name = \"luhua/chinese_pretrain_mrc_macbert_large\"\n",
        "#use model luhua/chinese_pretrain_mrc_macbert_large\n",
        "model = BertForQuestionAnswering.from_pretrained(model_name).to(device)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "#reference https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large\n",
        "\n",
        "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Td-GTmk5OW4"
      },
      "source": [
        "## Read Data\n",
        "\n",
        "- Training set: 26918 QA pairs\n",
        "- Dev set: 2863  QA pairs\n",
        "- Test set: 3524  QA pairs\n",
        "\n",
        "- {train/dev/test}_questions:\t\n",
        "  - List of dicts with the following keys:\n",
        "   - id (int)\n",
        "   - paragraph_id (int)\n",
        "   - question_text (string)\n",
        "   - answer_text (string)\n",
        "   - answer_start (int)\n",
        "   - answer_end (int)\n",
        "- {train/dev/test}_paragraphs: \n",
        "  - List of strings\n",
        "  - paragraph_ids in questions correspond to indexs in paragraphs\n",
        "  - A paragraph may be used by several questions "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NvX7hlepogvu"
      },
      "outputs": [],
      "source": [
        "def read_data(file):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
        "        data = json.load(reader)\n",
        "    return data[\"questions\"], data[\"paragraphs\"]\n",
        "\n",
        "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
        "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
        "test_questions, test_paragraphs = read_data(\"hw7_test.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm0rpTHq0e4N"
      },
      "source": [
        "## Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rTZ6B70Hoxie"
      },
      "outputs": [],
      "source": [
        "# Tokenize questions and paragraphs separately\n",
        "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
        "\n",
        "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
        "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
        "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
        "\n",
        "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
        "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
        "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
        "\n",
        "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8c8_4d5UCI"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xjooag-Swnuh"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 8\n",
        "doc_stride = 16\n",
        "\n",
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
        "        self.split = split\n",
        "        self.questions = questions\n",
        "        self.tokenized_questions = tokenized_questions\n",
        "        self.tokenized_paragraphs = tokenized_paragraphs\n",
        "        self.max_question_len = 60\n",
        "        self.max_paragraph_len = 150\n",
        "        \n",
        "        ##### TODO: Change value of doc_stride #####\n",
        "        self.doc_stride = 32\n",
        "\n",
        "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
        "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        tokenized_question = self.tokenized_questions[idx]\n",
        "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
        "\n",
        "        ##### TODO: Preprocessing #####\n",
        "        # Hint: How to prevent model from learning something it should not learn\n",
        "        if self.split == \"train\":\n",
        "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
        "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
        "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
        "\n",
        "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
        "            #mid = (answer_start_token + answer_end_token) // 2\n",
        "            #paragraph_start = max(0, min(mid - self.max_paragraph_len // 2, len(tokenized_paragraph) - self.max_paragraph_len))\n",
        "            #paragraph_end = paragraph_start + self.max_paragraph_len\n",
        "\n",
        "            start_min = max(0, answer_end_token - self.max_paragraph_len + 1)\n",
        "            start_max = min(answer_start_token, len(tokenized_paragraph) - self.max_paragraph_len)\n",
        "            start_max = max(start_min, start_max)\n",
        "            paragraph_start = random.randint(start_min, start_max + 1)\n",
        "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
        "\n",
        "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
        "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
        "            \n",
        "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
        "            answer_start_token += len(input_ids_question) - paragraph_start\n",
        "            answer_end_token += len(input_ids_question) - paragraph_start\n",
        "            \n",
        "            # Pad sequence and obtain inputs to model \n",
        "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
        "\n",
        "        # Validation/Testing\n",
        "        else:\n",
        "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
        "            \n",
        "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
        "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
        "                \n",
        "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
        "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
        "                \n",
        "                # Pad sequence and obtain inputs to model\n",
        "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "                \n",
        "                input_ids_list.append(input_ids)\n",
        "                token_type_ids_list.append(token_type_ids)\n",
        "                attention_mask_list.append(attention_mask)\n",
        "            \n",
        "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list)\n",
        "\n",
        "    def padding(self, input_ids_question, input_ids_paragraph):\n",
        "        # Pad zeros if sequence length is shorter than max_seq_len\n",
        "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
        "        # Indices of input sequence tokens in the vocabulary\n",
        "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
        "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
        "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
        "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
        "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
        "        \n",
        "        return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
        "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
        "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
        "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_H1kqhR8CdM"
      },
      "source": [
        "## Function for Evaluation\n",
        "postprocessing\n",
        "https://blog.csdn.net/qq_42994201/article/details/121442992\n",
        "\n",
        "unk handle\n",
        "https://blog.csdn.net/weixin_42369818/article/details/124835855?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168221931016800226541667%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168221931016800226541667&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-124835855-null-null.142^v86^insert_down1,239^v2^insert_chatgpt&utm_term=%E6%9D%8E%E5%AE%8F%E6%AF%85hw7&spm=1018.2226.3001.4187"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SqeA3PLPxOHu"
      },
      "outputs": [],
      "source": [
        "def evaluate(data, output, doc_stride=doc_stride, token_type_ids=None, paragraph=None, paragraph_tokenized=None):\n",
        "    ##### TODO: Postprocessing #####\n",
        "    # There is a bug and room for improvement in postprocessing \n",
        "    # Hint: Open your prediction file to see what is wrong \n",
        "    \n",
        "    answer = ''\n",
        "    max_prob = float('-inf')\n",
        "    num_of_windows = data[0].shape[1]\n",
        "    \n",
        "    for k in range(num_of_windows):\n",
        "        # Obtain answer by choosing the most probable start position / end position\n",
        "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
        "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
        "\n",
        "        token_type_id = data[1][0][k].detach().cpu().numpy()\n",
        "        #[CLS] + [question] + [SEP] + [paragraph] + [SEP]\n",
        "        paragraph_start = token_type_id.argmax()\n",
        "        paragraph_end = len(token_type_id) - 1 - token_type_id[::-1].argmax() - 1\n",
        "\n",
        "        #postprocessing ：https://blog.csdn.net/qq_42994201/article/details/121442992\n",
        "        if start_index > end_index or start_index < paragraph_start or end_index > paragraph_end:\n",
        "            continue\n",
        "        \n",
        "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
        "        prob = start_prob + end_prob\n",
        "        \n",
        "        # Replace answer if calculated probability is larger than previous windows\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
        "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
        "\n",
        "            # 找到tokenized paragraph中对应的位置\n",
        "            origin_start = start_index + k * doc_stride - paragraph_start\n",
        "            origin_end = end_index + k * doc_stride - paragraph_start\n",
        "    \n",
        "    answer = answer.replace(' ', '')\n",
        "    \n",
        "    if '[UNK]' in answer:\n",
        "        print('发现 [UNK]，这表明有文字无法编码, 使用原始文本')\n",
        "        #print(\"Paragraph:\", paragraph)\n",
        "        #print(\"Paragraph:\", paragraph_tokenized.tokens)\n",
        "        print('--直接解码预测:', answer)\n",
        "        #找到原始文本中对应的位置\n",
        "        raw_start = paragraph_tokenized.token_to_chars(origin_start)[0]\n",
        "        raw_end = paragraph_tokenized.token_to_chars(origin_end)[1]\n",
        "        answer = paragraph[raw_start:raw_end]\n",
        "        print('--原始文本预测:',answer)\n",
        "    #unk handle\n",
        "    #reference link： https://blog.csdn.net/weixin_42369818/article/details/124835855?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168221931016800226541667%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168221931016800226541667&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-124835855-null-null.142^v86^insert_down1,239^v2^insert_chatgpt&utm_term=%E6%9D%8E%E5%AE%8F%E6%AF%85hw7&spm=1018.2226.3001.4187\n",
        "    \n",
        "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
        "    return answer\n",
        "    # Remove spaces in answer (e.g. \"大 金\" --> \"大金\")\n",
        "    #return answer.replace(' ','')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzHQit6eMnKG"
      },
      "source": [
        "## Training\n",
        "#reference link\n",
        "\n",
        "https://blog.csdn.net/orangerfun/article/details/120400247\n",
        "\n",
        "https://gitee.com/yuys0602/NER-BERT-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3Q-B6ka7xoCM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780,
          "referenced_widgets": [
            "c443a30cb62e48098dc76f2ea5fe3b46",
            "0106ac6cf4dc4b4eaa1dfce75421816d",
            "2212083275424f33982dacc52c843de1",
            "24f143d1f9f54cc699b6798ad56a8286",
            "a84ec90507074dcc8fbd1c43583449c2",
            "246b1153e449447889d111b5a3a6d48d",
            "a1ff243991ed4622bd39defdd6879cb8",
            "4dba290693084e3db74d936c23f20462",
            "b944b318735e4e4fbe994ab1b12115db",
            "fb06c93160e94f15a03498653da1876b",
            "6f101cc1dcfb409b9031c09d6307a106"
          ]
        },
        "outputId": "47580c5a-772b-42d4-d34d-a225ca3158f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3723 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c443a30cb62e48098dc76f2ea5fe3b46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Step 100 | loss = 2.179, acc = 0.421\n",
            "Epoch 1 | Step 200 | loss = 1.228, acc = 0.577\n",
            "Epoch 1 | Step 300 | loss = 0.957, acc = 0.627\n",
            "Epoch 1 | Step 400 | loss = 0.851, acc = 0.640\n",
            "Epoch 1 | Step 500 | loss = 0.810, acc = 0.700\n",
            "Epoch 1 | Step 600 | loss = 0.809, acc = 0.644\n",
            "Epoch 1 | Step 700 | loss = 0.752, acc = 0.702\n",
            "Epoch 1 | Step 800 | loss = 0.716, acc = 0.689\n",
            "Epoch 1 | Step 900 | loss = 0.739, acc = 0.680\n",
            "Epoch 1 | Step 1000 | loss = 0.702, acc = 0.701\n",
            "Epoch 1 | Step 1100 | loss = 0.637, acc = 0.717\n",
            "Epoch 1 | Step 1200 | loss = 0.682, acc = 0.719\n",
            "Epoch 1 | Step 1300 | loss = 0.713, acc = 0.716\n",
            "Epoch 1 | Step 1400 | loss = 0.660, acc = 0.744\n",
            "Epoch 1 | Step 1500 | loss = 0.628, acc = 0.748\n",
            "Epoch 1 | Step 1600 | loss = 0.657, acc = 0.730\n",
            "Epoch 1 | Step 1700 | loss = 0.620, acc = 0.750\n",
            "Epoch 1 | Step 1800 | loss = 0.665, acc = 0.743\n",
            "Epoch 1 | Step 1900 | loss = 0.731, acc = 0.700\n",
            "Epoch 1 | Step 2000 | loss = 0.560, acc = 0.766\n",
            "Epoch 1 | Step 2100 | loss = 0.597, acc = 0.750\n",
            "Epoch 1 | Step 2200 | loss = 0.607, acc = 0.740\n",
            "Epoch 1 | Step 2300 | loss = 0.649, acc = 0.730\n",
            "Epoch 1 | Step 2400 | loss = 0.703, acc = 0.712\n",
            "Epoch 1 | Step 2500 | loss = 0.634, acc = 0.739\n",
            "Epoch 1 | Step 2600 | loss = 0.571, acc = 0.767\n",
            "Epoch 1 | Step 2700 | loss = 0.528, acc = 0.771\n",
            "Epoch 1 | Step 2800 | loss = 0.681, acc = 0.732\n",
            "Epoch 1 | Step 2900 | loss = 0.669, acc = 0.730\n",
            "Epoch 1 | Step 3000 | loss = 0.616, acc = 0.744\n",
            "Epoch 1 | Step 3100 | loss = 0.660, acc = 0.735\n",
            "Epoch 1 | Step 3200 | loss = 0.596, acc = 0.757\n",
            "Epoch 1 | Step 3300 | loss = 0.610, acc = 0.741\n",
            "Epoch 1 | Step 3400 | loss = 0.719, acc = 0.704\n",
            "Epoch 1 | Step 3500 | loss = 0.547, acc = 0.772\n",
            "Epoch 1 | Step 3600 | loss = 0.714, acc = 0.711\n",
            "Epoch 1 | Step 3700 | loss = 0.576, acc = 0.744\n",
            "Saving Model ...\n"
          ]
        }
      ],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "# hyperparameters\n",
        "num_epoch = 1\n",
        "validation = False\n",
        "logging_step = 100\n",
        "total_steps = num_epoch * len(train_loader)\n",
        "learning_rate = 1e-6\n",
        "acc_steps = 1\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "train_batch_size = 8\n",
        "\n",
        "#learning rate reference link: https://blog.csdn.net/orangerfun/article/details/120400247\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps//acc_steps)\n",
        "\n",
        "#### TODO: gradient_accumulation (optional)####\n",
        "# Note: train_batch_size * gradient_accumulation_steps = effective batch size\n",
        "# If CUDA out of memory, you can make train_batch_size lower and gradient_accumulation_steps upper\n",
        "# Doc: https://huggingface.co/docs/accelerate/usage_guides/gradient_accumulation\n",
        "gradient_accumulation_steps = 16\n",
        "\n",
        "# dataloader\n",
        "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
        "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
        "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
        "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "\n",
        "\n",
        "# Change \"fp16_training\" to True to support automatic mixed \n",
        "# precision training (fp16)\t\n",
        "fp16_training = True\n",
        "if fp16_training:    \n",
        "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
        "else:\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "\n",
        "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/\n",
        "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
        "\n",
        "model.train()\n",
        "\n",
        "#no validation： https://gitee.com/yuys0602/NER-BERT-pytorch/\n",
        "if not validation:\n",
        "    dev_set = QA_Dataset(\"train\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
        "    train_set = torch.utils.data.ConcatDataset([train_set, dev_set])\n",
        "    train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "print(\"Start Training ...\")\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    step = 1\n",
        "    train_loss = train_acc = 0\n",
        "    optimizer.zero_grad()\n",
        "    for data in tqdm(train_loader):\t\n",
        "        # Load all data into GPU\n",
        "        data = [i.to(device) for i in data]\n",
        "        \n",
        "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
        "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
        "        output = model(input_ids=data[0], token_type_ids=data[1], attention_mask=data[2], start_positions=data[3], end_positions=data[4])\n",
        "        # Choose the most probable start position / end position\n",
        "        start_index = torch.argmax(output.start_logits, dim=1)\n",
        "        end_index = torch.argmax(output.end_logits, dim=1)\n",
        "        \n",
        "        # Prediction is correct only if both start_index and end_index are correct\n",
        "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
        "           \n",
        "        train_loss += output.loss\n",
        "        \n",
        "        accelerator.backward(output.loss)\n",
        "        \n",
        "        step += 1\n",
        "        if step % acc_steps == 0:\n",
        "            #grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "        \n",
        "        ##### TODO: Apply linear learning rate decay #####\n",
        "\n",
        "        # Print training loss and accuracy over past logging step\n",
        "        if step % logging_step == 0:\n",
        "            lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
        "            train_loss = train_acc = 0\n",
        "\n",
        "    if validation:\n",
        "        print(\"Evaluating Dev Set ...\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            dev_acc = 0\n",
        "            for i, data in enumerate(tqdm(dev_loader)):\n",
        "                output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "                # prediction is correct only if answer text exactly matches\n",
        "                dev_acc += evaluate(data, output) == dev_questions[i][\"answer_text\"]\n",
        "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
        "        model.train()\n",
        "\n",
        "# Save a model and its configuration file to the directory 「saved_model」 \n",
        "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
        "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
        "print(\"Saving Model ...\")\n",
        "model_save_dir = \"saved_model\" \n",
        "model.save_pretrained(model_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMmdLOKBMsdE"
      },
      "source": [
        "## Testing\n",
        "\n",
        "# model reference\n",
        "https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5scNKC9xz0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cf4056421e3d4a42ba8ae2aa970e19ee",
            "98b3509978db4795822c31b2e6b27f2f",
            "728b5bacff4142e6a2ec6c1a2c9fbb25",
            "93cac88c5d30440fafb12a58276ae446",
            "d9d910eb218e4132b0c9a62d08d12008",
            "0759a0aa4dbd472599c49df8198fb804",
            "6b0b81d8238846ee9788c1063eea54f1",
            "8c96fdacb4664543bc67012007c28d21",
            "5a434869c31e4573b79111421ccd8fee",
            "db4189f08d404e1a9e979113e5a5d509",
            "dee899fb24af447e9b73710e4b92ab28"
          ]
        },
        "outputId": "10aec789-33a9-45a1-c637-5c10615f65c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Test Set ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3524 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf4056421e3d4a42ba8ae2aa970e19ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 回[UNK]\n",
            "--原始文本预测: 位年\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]\n",
            "--原始文本预测: 輸\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 回[UNK]汗國\n",
            "--原始文本预测: 回鶻汗國\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 白[UNK]紀中期\n",
            "--原始文本预测: 白堊紀中期\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]大壩\n",
            "--原始文本预测: 壩一般\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]\n",
            "--原始文本预测: 下\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 李[UNK]\n",
            "--原始文本预测: 李勣\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]\n",
            "--原始文本预测: 使\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 久彌宮妃[UNK]子\n",
            "--原始文本预测: 在戰爭中長大\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 《阿[UNK]婆吠陀》\n",
            "--原始文本预测: 》、《娑摩吠陀\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 姚[UNK]\n",
            "--原始文本预测: 北復\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]-850型\n",
            "--原始文本预测: 了三架本\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 分[UNK]期淺灌，幼穗分化開始間隙灌水\n",
            "--原始文本预测: 幼穗形成時，還有抽穗開花期加強水\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK][UNK]自動步槍系列\n",
            "--原始文本预测: 。MP5系列的發明把\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]水之戰\n",
            "--原始文本预测: 潰，北方\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 金[UNK]\n",
            "--原始文本预测: 鮮半\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 《[UNK]早餐》\n",
            "--原始文本预测: 場實況。由\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 白[UNK]紀\n",
            "--原始文本预测: 白堊紀\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 測試[UNK][UNK]村附近男性居民的脫氧核糖核酸\n",
            "--原始文本预测: 區的羅馬俘虜可能曾經與漢族士兵發生過\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 本田[UNK]型機動腳踏車\n",
            "--原始文本预测: 本田A型機動腳踏車\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 長[UNK]米\n",
            "--原始文本预测: 孟加拉\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 由於其中含有[UNK]\n",
            "--原始文本预测: 機，其廢氣對環\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]族\n",
            "--原始文本预测: 撣族\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]娛樂\n",
            "--原始文本预测: POP、搖\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 恭親王奕[UNK]\n",
            "--原始文本预测: 、太僕寺少\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: 蔡[UNK]\n",
            "--原始文本预测: 蔡鍔\n",
            "发现 [UNK]，这表明有文字无法编码, 使用原始文本\n",
            "--直接解码预测: [UNK]-61戰鬥機\n",
            "--原始文本预测: 傷，發現敵機\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating Test Set ...\")\n",
        "\n",
        "result = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    #for data in tqdm(test_loader):\n",
        "    #new model need： https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large\n",
        "    for i, data in enumerate(tqdm(test_loader)):\n",
        "        #output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       #attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        #result.append(evaluate(data, output))\n",
        "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        result.append(evaluate(data, output, doc_stride=doc_stride, paragraph=test_paragraphs[test_questions[i][\"paragraph_id\"]],\n",
        "                               paragraph_tokenized=test_paragraphs_tokenized[test_questions[i][\"paragraph_id\"]]))\n",
        "\n",
        "result_file = \"result.csv\"\n",
        "with open(result_file, 'w') as f:\t\n",
        "    f.write(\"ID,Answer\\n\")\n",
        "    for i, test_question in enumerate(test_questions):\n",
        "    # Replace commas in answers with empty strings (since csv is separated by comma)\n",
        "    # Answers in kaggle are processed in the same way\n",
        "        f.write(f\"{test_question['id']},{result[i].replace(',','')}\\n\")\n",
        "\n",
        "print(f\"Completed! Result is in {result_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GradeScope - Question 2 (In-context learning)"
      ],
      "metadata": {
        "id": "0_JVNKOCywbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In-context learning\n",
        "The example prompt is :\n",
        "```\n",
        "請從最後一篇的文章中找出最後一個問題的答案：\n",
        "文章：<文章1 內容>\n",
        "問題：<問題1 敘述>\n",
        "答案：<答案1>\n",
        "...\n",
        "文章：<文章n 內容>\n",
        "問題：<問題n 敘述>\n",
        "答案：\n",
        "```"
      ],
      "metadata": {
        "id": "SDYN85why7zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random  \n",
        "import numpy as np\n",
        "\n",
        "# To avoid CUDA_OUT_OF_MEMORY\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "def same_seeds(seed):\n",
        "\ttorch.manual_seed(seed)\n",
        "\tif torch.cuda.is_available():\n",
        "\t\t\ttorch.cuda.manual_seed(seed)\n",
        "\t\t\ttorch.cuda.manual_seed_all(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\trandom.seed(seed)\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "same_seeds(2)"
      ],
      "metadata": {
        "id": "sGUxltxqzKpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# You can try model with different size\n",
        "# When using Colab or Kaggle, models with more than 2 billions parameters may \n",
        "# run out of memory\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/xglm-1.7B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/xglm-1.7B\")"
      ],
      "metadata": {
        "id": "wIS23s79zItf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To clean model output. If you try different prompts, you may have to fix \n",
        "# this function on your own\n",
        "def clean_text(text):\n",
        "    # Note: When you use unilingual model, the colon may become fullwidth\n",
        "    text = text.split(\"答案:\")[-1]\n",
        "    text = text.split(\" \")[0]\n",
        "    return text"
      ],
      "metadata": {
        "id": "2d97YtavzFHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "with open(\"hw7_in-context-learning-examples.json\", \"r\") as f: \n",
        "    test = json.load(f)\n",
        "\n",
        "# K-shot learning \n",
        "# Give model K examples to make it achieve better accuracy \n",
        "# Note: (1) When K >= 4, CUDA_OUT_OFF_MEMORY may occur.\n",
        "#       (2) The maximum input length of XGLM is 2048\n",
        "K = 2\n",
        "\n",
        "question_ids = [qa[\"id\"] for qa in test[\"questions\"]]\n",
        "\n",
        "with open(\"in-context-learning-result.txt\", \"w\") as f:\n",
        "    print(\"ID,Ground-Truth,Prediction\", file = f)\n",
        "    with torch.no_grad():\n",
        "        for idx, qa in enumerate(test[\"questions\"]):\n",
        "            # You can try different prompts\n",
        "            #prompt = \"請從最後一篇的文章中找出最後一個問題的答案\\n\" #normal prompt\n",
        "            prompt = \"答案限制在兩句話內，請根據最後一篇文章回答最後一個問題\\n\"  # Sentence structure restriction\n",
        "            #prompt = \"答案限制在10個字內，請根據最後一篇文章回答最後一個問題\\n\"  # WORD LIMIT\n",
        "            exist_question_indexs = [question_ids.index(qa[\"id\"])]\n",
        "\n",
        "            # K-shot learning: give the model K examples with answers\n",
        "            for i in range(K):\n",
        "                question_index = question_ids.index(qa[\"id\"])\n",
        "                while(question_index in exist_question_indexs): \n",
        "                    question_index = random.randint(0, len(question_ids) - 1)\n",
        "                exist_question_indexs.append(question_index)    \n",
        "                paragraph_id = test[\"questions\"][question_index][\"paragraph_id\"]\n",
        "                prompt += f'文章：{test[\"paragraphs\"][paragraph_id]}\\n'\n",
        "                prompt += f'問題：{test[\"questions\"][question_index][\"question_text\"]}\\n'\n",
        "                prompt += f'答案：{test[\"questions\"][question_index][\"answer_text\"]}\\n'\n",
        "\n",
        "            # The final one question without answer\n",
        "            paragraph_id = qa[\"paragraph_id\"]\n",
        "            prompt += f'文章：{test[\"paragraphs\"][paragraph_id]}\\n'\n",
        "            prompt += f'問題：{qa[\"question_text\"]}\\n'\n",
        "            prompt += f'答案：'\n",
        "            \n",
        "            inputs = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\") \n",
        "            sample = model.generate(**inputs, max_new_tokens = 20)\n",
        "            text = tokenizer.decode(sample[0], skip_special_tokens=True)\n",
        "\n",
        "            # Note: You can delete this line to see what will happen\n",
        "            text = clean_text(text)\n",
        "            \n",
        "            print(prompt)\n",
        "            print(f'正確答案: {qa[\"answer_text\"]}')\n",
        "            print(f'模型輸出: {text}')\n",
        "            print()\n",
        "\n",
        "            print(f\"{idx},{qa['answer_text']},{text}\", file = f)"
      ],
      "metadata": {
        "id": "jdf3gdP_yykH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3prlGCdjf38"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c443a30cb62e48098dc76f2ea5fe3b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0106ac6cf4dc4b4eaa1dfce75421816d",
              "IPY_MODEL_2212083275424f33982dacc52c843de1",
              "IPY_MODEL_24f143d1f9f54cc699b6798ad56a8286"
            ],
            "layout": "IPY_MODEL_a84ec90507074dcc8fbd1c43583449c2"
          }
        },
        "0106ac6cf4dc4b4eaa1dfce75421816d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_246b1153e449447889d111b5a3a6d48d",
            "placeholder": "​",
            "style": "IPY_MODEL_a1ff243991ed4622bd39defdd6879cb8",
            "value": "100%"
          }
        },
        "2212083275424f33982dacc52c843de1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4dba290693084e3db74d936c23f20462",
            "max": 3723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b944b318735e4e4fbe994ab1b12115db",
            "value": 3723
          }
        },
        "24f143d1f9f54cc699b6798ad56a8286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb06c93160e94f15a03498653da1876b",
            "placeholder": "​",
            "style": "IPY_MODEL_6f101cc1dcfb409b9031c09d6307a106",
            "value": " 3723/3723 [29:54&lt;00:00,  2.25it/s]"
          }
        },
        "a84ec90507074dcc8fbd1c43583449c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246b1153e449447889d111b5a3a6d48d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1ff243991ed4622bd39defdd6879cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dba290693084e3db74d936c23f20462": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b944b318735e4e4fbe994ab1b12115db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb06c93160e94f15a03498653da1876b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f101cc1dcfb409b9031c09d6307a106": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf4056421e3d4a42ba8ae2aa970e19ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98b3509978db4795822c31b2e6b27f2f",
              "IPY_MODEL_728b5bacff4142e6a2ec6c1a2c9fbb25",
              "IPY_MODEL_93cac88c5d30440fafb12a58276ae446"
            ],
            "layout": "IPY_MODEL_d9d910eb218e4132b0c9a62d08d12008"
          }
        },
        "98b3509978db4795822c31b2e6b27f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0759a0aa4dbd472599c49df8198fb804",
            "placeholder": "​",
            "style": "IPY_MODEL_6b0b81d8238846ee9788c1063eea54f1",
            "value": " 78%"
          }
        },
        "728b5bacff4142e6a2ec6c1a2c9fbb25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c96fdacb4664543bc67012007c28d21",
            "max": 3524,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a434869c31e4573b79111421ccd8fee",
            "value": 2743
          }
        },
        "93cac88c5d30440fafb12a58276ae446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4189f08d404e1a9e979113e5a5d509",
            "placeholder": "​",
            "style": "IPY_MODEL_dee899fb24af447e9b73710e4b92ab28",
            "value": " 2743/3524 [09:49&lt;02:32,  5.14it/s]"
          }
        },
        "d9d910eb218e4132b0c9a62d08d12008": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0759a0aa4dbd472599c49df8198fb804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b0b81d8238846ee9788c1063eea54f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c96fdacb4664543bc67012007c28d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a434869c31e4573b79111421ccd8fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db4189f08d404e1a9e979113e5a5d509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee899fb24af447e9b73710e4b92ab28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}